{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF 文件萃取與向量化流程\n",
    "\n",
    "## 資料處理流程\n",
    "- 使用 OpenAI API 萃取 PDF 文件結構化內容\n",
    "- 將結構化內容轉換為向量表示\n",
    "- 使用 FAISS 建立向量資料庫儲存\n",
    "\n",
    "## 步驟\n",
    "1. PDF 內容萃取\n",
    "    - 使用 OpenAI API 進行文本結構化\n",
    "    - 將內容轉換為標準 JSON 格式\n",
    "2. 向量化處理\n",
    "    - 建立 FAISS 向量資料庫\n",
    "    - 將文件內容轉換為向量\n",
    "3. 資料儲存與索引\n",
    "    - 儲存向量索引\n",
    "    - 建立文件檢索機制\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  fitz\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    \"\"\"讀取 PDF 檔案並依據文本頁數返回其內容\"\"\"\n",
    "    page_content= []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        page_content.append({'page':page_num+1,'content':text})\n",
    "    doc.close()\n",
    "    return page_content\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "你是一個專業的文本標記助手，請根據以下文本內容，在適當的位置加上標記，並返回 **有效的 JSON 格式** 結果。\n",
    "### 文本內容：\n",
    "    {content}\n",
    "### **請使用以下標記類型**\n",
    "- `\"TITLE\"`：章節標題\n",
    "- `\"TEXT\"`：正文\n",
    "- `\"TABLE\"`：表格（表格應以 JSON 陣列表示，每一行包含 `label` 和 `value`）\n",
    "\n",
    "### **請確保**\n",
    "1. **保持原始文本內容**，僅添加標記，不要修改任何文字。\n",
    "2. **輸出結果必須是 JSON 格式**，**不能包含額外的說明**。\n",
    "3. **JSON 結構必須符合以下格式**：\n",
    "```json\n",
    "{{\n",
    "    \"document\": [\n",
    "        {{\n",
    "            \"type\": \"TITLE\",\n",
    "            \"content\": \"標題文字\"\n",
    "        }},\n",
    "        {{\n",
    "            \"type\": \"TEXT\",\n",
    "            \"content\": \"正文內容...\"\n",
    "        }},\n",
    "        {{\n",
    "            \"type\": \"TABLE\",\n",
    "            \"content\": [\n",
    "                {{\n",
    "                    \"label\": \"欄位名稱\",\n",
    "                    \"value\": \"對應值\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../pdf'\n",
    "question_list = []\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]\n",
    "\n",
    "api_key = \"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\"\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file_name in pdf_files:\n",
    "    file_data = {\n",
    "        \"file_name\": file_name,\n",
    "        \"pages\": []\n",
    "    }\n",
    "    pdf_content = read_pdf(folder_path + '/' + file_name)\n",
    "    for page in pdf_content:\n",
    "        # Remove header text\n",
    "        page['content'] = page['content'].replace('臺灣學術網路危機處理中心(TACERT) \\nTANet Computer Emergency Response Team  ','')\n",
    "        \n",
    "        # Create prompt and get completion\n",
    "        prompt = prompt_template.format(content=page['content'])\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0.2,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Parse JSON response\n",
    "        json_str = completion.choices[0].message.content.strip().strip('```json\\n').strip('```')\n",
    "        parsed_json = json.loads(json_str)\n",
    "        \n",
    "        # Add page data\n",
    "        page_data = {\n",
    "            \"page_number\": page['page'],\n",
    "            \"content\": parsed_json\n",
    "        }\n",
    "        file_data[\"pages\"].append(page_data) \n",
    "    data.append(file_data)\n",
    "\n",
    "\n",
    "# Save data to JSON file with proper formatting\n",
    "with open('unstracted_pdf.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 向量化處理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': 'csoperosn.pdf', 'sections': [{'content': '資安通報平台資安長資訊說明依據資通安全管理法第11 條規定:資通安全長由機關首長指派副首長或適當人員兼任，負責推動及監督機關內資通安全相關事務。1. 請單位的資安連絡人登錄「教育機構資安通報平台」(https://info.cert.tanet.edu.tw)後，點選左方功能列(圖1)之「修改資安長資料」功能項。圖1. 教育機構資安通報平台左方功能列2. 輸入單位的資安長相關連絡資訊(包含資安長姓名、資安長公務電話及資安長公務e-mail)，點選「送出」以儲存資料。圖2.修改資安長資訊功能截圖', 'Page_Num': [1]}]}\n"
     ]
    }
   ],
   "source": [
    "with open('unstracted_pdf.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "for file in data:\n",
    "    file_name = file['file_name']\n",
    "    current_title = ''\n",
    "    current_content = []\n",
    "    file_content = []\n",
    "    page_numbers = set()  # Using set to track unique page numbers\n",
    "    section_page_numbers = set()  # Track pages for current section\n",
    "\n",
    "    for page in file['pages']:\n",
    "        current_page = page['page_number']\n",
    "        \n",
    "        for item in page['content']['document']:\n",
    "            if item['type'] == 'TITLE':\n",
    "                # If we have a previous section, save it with its page numbers\n",
    "                if current_title and current_content:\n",
    "                    file_content.append({\n",
    "                        'content':  \"\".join(current_content),\n",
    "                        'Page_Num': list(section_page_numbers)  # Convert set to list\n",
    "                    })\n",
    "                \n",
    "                # Start new section\n",
    "                current_title = item['content']\n",
    "                current_content = [item['content']]\n",
    "                section_page_numbers = {current_page}  # Initialize with current page\n",
    "                \n",
    "            elif item['type'] == 'TABLE':\n",
    "                temp = item['content'] \n",
    "                for tablelist in temp:\n",
    "                    current_content.append(str(tablelist['label']) + str(tablelist['value']))\n",
    "                section_page_numbers.add(current_page)\n",
    "            else:\n",
    "                current_content.append(item['content'])\n",
    "                section_page_numbers.add(current_page)\n",
    "\n",
    "    # Don't forget to add the last section\n",
    "    if current_title and current_content:\n",
    "        file_content.append({\n",
    "            'content': \"\".join(current_content),\n",
    "            'Page_Num': list(section_page_numbers)\n",
    "        })\n",
    "    \n",
    "    processed_data.append({\n",
    "        'file_name': file_name,\n",
    "        'sections': file_content\n",
    "    })\n",
    "\n",
    "\n",
    "print(processed_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:371: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "# 初始化嵌入模型\n",
    "embed_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "EMBEDDING_DIM = 1024  # **請根據你的 embedding 模型改變這個數值**\n",
    "index = faiss.IndexFlatL2(EMBEDDING_DIM)  # L2 距離索引\n",
    "faiss_metadata = {}\n",
    "\n",
    "\n",
    "for pdf_file in processed_data:\n",
    "    file_name = pdf_file['file_name']\n",
    "    for idx, chunk in enumerate(pdf_file['sections']):\n",
    "        text = chunk[\"content\"]\n",
    "\n",
    "        \n",
    "        # **🚀 計算 embedding 並存入 cache**\n",
    "        embedding = embed_model.encode(text).astype(np.float32)\n",
    "\n",
    "        # **🚀 儲存 metadata**\n",
    "        data.append({\n",
    "            \"File_Name\": file_name,\n",
    "            \"content\": text,\n",
    "            \"Page_Num\": ','.join(str(p) for p in chunk[\"Page_Num\"])\n",
    "        })\n",
    "        index.add(np.array([embedding]))  # 加入 FAISS\n",
    "        faiss_metadata[len(faiss_metadata)] = data[-1]  # FAISS ID 對應 metadata\n",
    "    \n",
    "faiss.write_index(index, \"faiss_element_chunk_index.idx\")\n",
    "with open(\"faiss_element_chunk_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(faiss_metadata, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
