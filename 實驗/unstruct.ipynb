{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF æ–‡ä»¶èƒå–èˆ‡å‘é‡åŒ–æµç¨‹\n",
    "\n",
    "## è³‡æ–™è™•ç†æµç¨‹\n",
    "- ä½¿ç”¨ OpenAI API èƒå– PDF æ–‡ä»¶çµæ§‹åŒ–å…§å®¹\n",
    "- å°‡çµæ§‹åŒ–å…§å®¹è½‰æ›ç‚ºå‘é‡è¡¨ç¤º\n",
    "- ä½¿ç”¨ FAISS å»ºç«‹å‘é‡è³‡æ–™åº«å„²å­˜\n",
    "\n",
    "## æ­¥é©Ÿ\n",
    "1. PDF å…§å®¹èƒå–\n",
    "    - ä½¿ç”¨ OpenAI API é€²è¡Œæ–‡æœ¬çµæ§‹åŒ–\n",
    "    - å°‡å…§å®¹è½‰æ›ç‚ºæ¨™æº– JSON æ ¼å¼\n",
    "2. å‘é‡åŒ–è™•ç†\n",
    "    - å»ºç«‹ FAISS å‘é‡è³‡æ–™åº«\n",
    "    - å°‡æ–‡ä»¶å…§å®¹è½‰æ›ç‚ºå‘é‡\n",
    "3. è³‡æ–™å„²å­˜èˆ‡ç´¢å¼•\n",
    "    - å„²å­˜å‘é‡ç´¢å¼•\n",
    "    - å»ºç«‹æ–‡ä»¶æª¢ç´¢æ©Ÿåˆ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  fitz\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    \"\"\"è®€å– PDF æª”æ¡ˆä¸¦ä¾æ“šæ–‡æœ¬é æ•¸è¿”å›å…¶å…§å®¹\"\"\"\n",
    "    page_content= []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        page_content.append({'page':page_num+1,'content':text})\n",
    "    doc.close()\n",
    "    return page_content\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬æ¨™è¨˜åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æ–‡æœ¬å…§å®¹ï¼Œåœ¨é©ç•¶çš„ä½ç½®åŠ ä¸Šæ¨™è¨˜ï¼Œä¸¦è¿”å› **æœ‰æ•ˆçš„ JSON æ ¼å¼** çµæœã€‚\n",
    "### æ–‡æœ¬å…§å®¹ï¼š\n",
    "    {content}\n",
    "### **è«‹ä½¿ç”¨ä»¥ä¸‹æ¨™è¨˜é¡å‹**\n",
    "- `\"TITLE\"`ï¼šç« ç¯€æ¨™é¡Œ\n",
    "- `\"TEXT\"`ï¼šæ­£æ–‡\n",
    "- `\"TABLE\"`ï¼šè¡¨æ ¼ï¼ˆè¡¨æ ¼æ‡‰ä»¥ JSON é™£åˆ—è¡¨ç¤ºï¼Œæ¯ä¸€è¡ŒåŒ…å« `label` å’Œ `value`ï¼‰\n",
    "\n",
    "### **è«‹ç¢ºä¿**\n",
    "1. **ä¿æŒåŸå§‹æ–‡æœ¬å…§å®¹**ï¼Œåƒ…æ·»åŠ æ¨™è¨˜ï¼Œä¸è¦ä¿®æ”¹ä»»ä½•æ–‡å­—ã€‚\n",
    "2. **è¼¸å‡ºçµæœå¿…é ˆæ˜¯ JSON æ ¼å¼**ï¼Œ**ä¸èƒ½åŒ…å«é¡å¤–çš„èªªæ˜**ã€‚\n",
    "3. **JSON çµæ§‹å¿…é ˆç¬¦åˆä»¥ä¸‹æ ¼å¼**ï¼š\n",
    "```json\n",
    "{{\n",
    "    \"document\": [\n",
    "        {{\n",
    "            \"type\": \"TITLE\",\n",
    "            \"content\": \"æ¨™é¡Œæ–‡å­—\"\n",
    "        }},\n",
    "        {{\n",
    "            \"type\": \"TEXT\",\n",
    "            \"content\": \"æ­£æ–‡å…§å®¹...\"\n",
    "        }},\n",
    "        {{\n",
    "            \"type\": \"TABLE\",\n",
    "            \"content\": [\n",
    "                {{\n",
    "                    \"label\": \"æ¬„ä½åç¨±\",\n",
    "                    \"value\": \"å°æ‡‰å€¼\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../pdf'\n",
    "question_list = []\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]\n",
    "\n",
    "api_key = \"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\"\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file_name in pdf_files:\n",
    "    file_data = {\n",
    "        \"file_name\": file_name,\n",
    "        \"pages\": []\n",
    "    }\n",
    "    pdf_content = read_pdf(folder_path + '/' + file_name)\n",
    "    for page in pdf_content:\n",
    "        # Remove header text\n",
    "        page['content'] = page['content'].replace('è‡ºç£å­¸è¡“ç¶²è·¯å±æ©Ÿè™•ç†ä¸­å¿ƒ(TACERT) \\nTANet Computer Emergency Response Team  ','')\n",
    "        \n",
    "        # Create prompt and get completion\n",
    "        prompt = prompt_template.format(content=page['content'])\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0.2,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Parse JSON response\n",
    "        json_str = completion.choices[0].message.content.strip().strip('```json\\n').strip('```')\n",
    "        parsed_json = json.loads(json_str)\n",
    "        \n",
    "        # Add page data\n",
    "        page_data = {\n",
    "            \"page_number\": page['page'],\n",
    "            \"content\": parsed_json\n",
    "        }\n",
    "        file_data[\"pages\"].append(page_data) \n",
    "    data.append(file_data)\n",
    "\n",
    "\n",
    "# Save data to JSON file with proper formatting\n",
    "with open('unstracted_pdf.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# å‘é‡åŒ–è™•ç†æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': 'csoperosn.pdf', 'sections': [{'content': 'è³‡å®‰é€šå ±å¹³å°è³‡å®‰é•·è³‡è¨Šèªªæ˜ä¾æ“šè³‡é€šå®‰å…¨ç®¡ç†æ³•ç¬¬11 æ¢è¦å®š:è³‡é€šå®‰å…¨é•·ç”±æ©Ÿé—œé¦–é•·æŒ‡æ´¾å‰¯é¦–é•·æˆ–é©ç•¶äººå“¡å…¼ä»»ï¼Œè² è²¬æ¨å‹•åŠç›£ç£æ©Ÿé—œå…§è³‡é€šå®‰å…¨ç›¸é—œäº‹å‹™ã€‚1. è«‹å–®ä½çš„è³‡å®‰é€£çµ¡äººç™»éŒ„ã€Œæ•™è‚²æ©Ÿæ§‹è³‡å®‰é€šå ±å¹³å°ã€(https://info.cert.tanet.edu.tw)å¾Œï¼Œé»é¸å·¦æ–¹åŠŸèƒ½åˆ—(åœ–1)ä¹‹ã€Œä¿®æ”¹è³‡å®‰é•·è³‡æ–™ã€åŠŸèƒ½é …ã€‚åœ–1. æ•™è‚²æ©Ÿæ§‹è³‡å®‰é€šå ±å¹³å°å·¦æ–¹åŠŸèƒ½åˆ—2. è¼¸å…¥å–®ä½çš„è³‡å®‰é•·ç›¸é—œé€£çµ¡è³‡è¨Š(åŒ…å«è³‡å®‰é•·å§“åã€è³‡å®‰é•·å…¬å‹™é›»è©±åŠè³‡å®‰é•·å…¬å‹™e-mail)ï¼Œé»é¸ã€Œé€å‡ºã€ä»¥å„²å­˜è³‡æ–™ã€‚åœ–2.ä¿®æ”¹è³‡å®‰é•·è³‡è¨ŠåŠŸèƒ½æˆªåœ–', 'Page_Num': [1]}]}\n"
     ]
    }
   ],
   "source": [
    "with open('unstracted_pdf.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "for file in data:\n",
    "    file_name = file['file_name']\n",
    "    current_title = ''\n",
    "    current_content = []\n",
    "    file_content = []\n",
    "    page_numbers = set()  # Using set to track unique page numbers\n",
    "    section_page_numbers = set()  # Track pages for current section\n",
    "\n",
    "    for page in file['pages']:\n",
    "        current_page = page['page_number']\n",
    "        \n",
    "        for item in page['content']['document']:\n",
    "            if item['type'] == 'TITLE':\n",
    "                # If we have a previous section, save it with its page numbers\n",
    "                if current_title and current_content:\n",
    "                    file_content.append({\n",
    "                        'content':  \"\".join(current_content),\n",
    "                        'Page_Num': list(section_page_numbers)  # Convert set to list\n",
    "                    })\n",
    "                \n",
    "                # Start new section\n",
    "                current_title = item['content']\n",
    "                current_content = [item['content']]\n",
    "                section_page_numbers = {current_page}  # Initialize with current page\n",
    "                \n",
    "            elif item['type'] == 'TABLE':\n",
    "                temp = item['content'] \n",
    "                for tablelist in temp:\n",
    "                    current_content.append(str(tablelist['label']) + str(tablelist['value']))\n",
    "                section_page_numbers.add(current_page)\n",
    "            else:\n",
    "                current_content.append(item['content'])\n",
    "                section_page_numbers.add(current_page)\n",
    "\n",
    "    # Don't forget to add the last section\n",
    "    if current_title and current_content:\n",
    "        file_content.append({\n",
    "            'content': \"\".join(current_content),\n",
    "            'Page_Num': list(section_page_numbers)\n",
    "        })\n",
    "    \n",
    "    processed_data.append({\n",
    "        'file_name': file_name,\n",
    "        'sections': file_content\n",
    "    })\n",
    "\n",
    "\n",
    "print(processed_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:371: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "embed_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "EMBEDDING_DIM = 1024  # **è«‹æ ¹æ“šä½ çš„ embedding æ¨¡å‹æ”¹è®Šé€™å€‹æ•¸å€¼**\n",
    "index = faiss.IndexFlatL2(EMBEDDING_DIM)  # L2 è·é›¢ç´¢å¼•\n",
    "faiss_metadata = {}\n",
    "\n",
    "\n",
    "for pdf_file in processed_data:\n",
    "    file_name = pdf_file['file_name']\n",
    "    for idx, chunk in enumerate(pdf_file['sections']):\n",
    "        text = chunk[\"content\"]\n",
    "\n",
    "        \n",
    "        # **ğŸš€ è¨ˆç®— embedding ä¸¦å­˜å…¥ cache**\n",
    "        embedding = embed_model.encode(text).astype(np.float32)\n",
    "\n",
    "        # **ğŸš€ å„²å­˜ metadata**\n",
    "        data.append({\n",
    "            \"File_Name\": file_name,\n",
    "            \"content\": text,\n",
    "            \"Page_Num\": ','.join(str(p) for p in chunk[\"Page_Num\"])\n",
    "        })\n",
    "        index.add(np.array([embedding]))  # åŠ å…¥ FAISS\n",
    "        faiss_metadata[len(faiss_metadata)] = data[-1]  # FAISS ID å°æ‡‰ metadata\n",
    "    \n",
    "faiss.write_index(index, \"faiss_element_chunk_index.idx\")\n",
    "with open(\"faiss_element_chunk_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(faiss_metadata, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
