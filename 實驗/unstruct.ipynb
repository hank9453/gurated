{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF æ–‡ä»¶èƒå–èˆ‡å‘é‡åŒ–æµç¨‹\n",
    "\n",
    "## è³‡æ–™è™•ç†æµç¨‹\n",
    "- ä½¿ç”¨ OpenAI API èƒå– PDF æ–‡ä»¶çµæ§‹åŒ–å…§å®¹\n",
    "- å°‡çµæ§‹åŒ–å…§å®¹è½‰æ›ç‚ºå‘é‡è¡¨ç¤º\n",
    "- ä½¿ç”¨ FAISS å»ºç«‹å‘é‡è³‡æ–™åº«å„²å­˜\n",
    "\n",
    "## æ­¥é©Ÿ\n",
    "1. PDF å…§å®¹èƒå–\n",
    "    - ä½¿ç”¨ OpenAI API é€²è¡Œæ–‡æœ¬çµæ§‹åŒ–\n",
    "    - å°‡å…§å®¹è½‰æ›ç‚ºæ¨™æº– JSON æ ¼å¼\n",
    "2. å‘é‡åŒ–è™•ç†\n",
    "    - å»ºç«‹ FAISS å‘é‡è³‡æ–™åº«\n",
    "    - å°‡æ–‡ä»¶å…§å®¹è½‰æ›ç‚ºå‘é‡\n",
    "3. è³‡æ–™å„²å­˜èˆ‡ç´¢å¼•\n",
    "    - å„²å­˜å‘é‡ç´¢å¼•\n",
    "    - å»ºç«‹æ–‡ä»¶æª¢ç´¢æ©Ÿåˆ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  fitz\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    \"\"\"è®€å– PDF æª”æ¡ˆä¸¦ä¾æ“šæ–‡æœ¬é æ•¸è¿”å›å…¶å…§å®¹\"\"\"\n",
    "    page_content= []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        page_content.append({'page':page_num+1,'content':text})\n",
    "    doc.close()\n",
    "    return page_content\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬æ¨™è¨˜åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æ–‡æœ¬å…§å®¹ï¼Œåœ¨é©ç•¶çš„ä½ç½®åŠ ä¸Šæ¨™è¨˜ï¼Œä¸¦è¿”å› **æœ‰æ•ˆçš„ JSON æ ¼å¼** çµæœã€‚\n",
    "### æ–‡æœ¬å…§å®¹ï¼š\n",
    "    {content}\n",
    "\n",
    "### **è«‹ä½¿ç”¨ä»¥ä¸‹æ¨™è¨˜é¡å‹**\n",
    "- `\"TITLE\"`ï¼šç« ç¯€æ¨™é¡Œ\n",
    "- `\"TEXT\"`ï¼šæ­£æ–‡\n",
    "### **è«‹ç¢ºä¿**\n",
    "1. **ä¿æŒåŸå§‹æ–‡æœ¬å…§å®¹**ï¼Œåƒ…æ·»åŠ æ¨™è¨˜ï¼Œä¸è¦ä¿®æ”¹ä»»ä½•æ–‡å­—ã€‚  \n",
    "2. **æ¯å€‹æ®µè½ä½œç‚ºä¸€å€‹ç¨ç«‹çš„å€å¡Šï¼Œæ¨™è¨˜ç‚º `\"TITLE\"` æˆ– `\"TEXT\"`**ã€‚  \n",
    "3. **è‹¥å…§å®¹ç‚ºæ¨™é¡Œï¼Œæ¨™è¨˜ç‚º `\"TITLE\"`ï¼Œå…¶é¤˜å…§å®¹æ¨™è¨˜ç‚º `\"TEXT\"`**ã€‚  \n",
    "4. **`\"TEXT\"` èˆ‡ `\"TITLE\"` å…§å®¹éœ€ä¿æŒä¸€è‡´**ï¼Œä¸å¯éºæ¼æˆ–åˆªæ¸›ã€‚  \n",
    "5. **è¼¸å‡ºçµæœå¿…é ˆæ˜¯ JSON æ ¼å¼**ï¼Œä¸”ç¬¦åˆä»¥ä¸‹çµæ§‹ï¼š  \n",
    "\n",
    "```json\n",
    "{{\n",
    "    \"document\": [\n",
    "        {{\n",
    "            \"type\": \"TITLE\",\n",
    "            \"content\": \"æ¨™é¡Œæ–‡å­—\"\n",
    "        }},\n",
    "        {{\n",
    "            \"type\": \"TEXT\",\n",
    "            \"content\": \"æ­£æ–‡å…§å®¹...\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../pdf'\n",
    "question_list = []\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]\n",
    "\n",
    "api_key = \"\"\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file_name in pdf_files:\n",
    "    file_data = {\n",
    "        \"file_name\": file_name,\n",
    "        \"pages\": []\n",
    "    }\n",
    "    pdf_content = read_pdf(folder_path + '/' + file_name)\n",
    "    for page in pdf_content:\n",
    "        # Remove header text\n",
    "        page['content'] = page['content'].replace('è‡ºç£å­¸è¡“ç¶²è·¯å±æ©Ÿè™•ç†ä¸­å¿ƒ(TACERT) \\nTANet Computer Emergency Response Team  ','')\n",
    "        \n",
    "        # Create prompt and get completion\n",
    "        prompt = prompt_template.format(content=page['content'])\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0.2,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Parse JSON response\n",
    "        json_str = completion.choices[0].message.content.strip().strip('```json\\n').strip('```')\n",
    "        parsed_json = json.loads(json_str)\n",
    "        \n",
    "        # Add page data\n",
    "        page_data = {\n",
    "            \"page_number\": page['page'],\n",
    "            \"content\": parsed_json\n",
    "        }\n",
    "        file_data[\"pages\"].append(page_data) \n",
    "    data.append(file_data)\n",
    "\n",
    "\n",
    "# Save data to JSON file with proper formatting\n",
    "with open('unstracted_pdf.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# å‘é‡åŒ–è™•ç†æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unstracted_pdf.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "for file in data:\n",
    "    file_name = file['file_name']\n",
    "    current_title = ''\n",
    "    current_content = []\n",
    "    file_content = []\n",
    "    page_numbers = set()  # Using set to track unique page numbers\n",
    "    section_page_numbers = set()  # Track pages for current section\n",
    "\n",
    "    for page in file['pages']:\n",
    "        current_page = page['page_number']\n",
    "        \n",
    "        for item in page['content']['document']:\n",
    "            if item['type'] == 'TITLE':\n",
    "                # If we have a previous section, save it with its page numbers\n",
    "                if current_title and current_content:\n",
    "                    file_content.append({\n",
    "                        'content':  \"\".join(current_content),\n",
    "                        'Page_Num': list(section_page_numbers)  # Convert set to list\n",
    "                    })             \n",
    "                # Start new section\n",
    "                current_title = item['content']\n",
    "                current_content = [item['content']]\n",
    "                section_page_numbers = {current_page}  # Initialize with current page\n",
    "            else:\n",
    "                current_content.append(item['content'])\n",
    "                section_page_numbers.add(current_page)\n",
    "\n",
    "    # Don't forget to add the last section\n",
    "    if current_title and current_content:\n",
    "        file_content.append({\n",
    "            'content': \"\".join(current_content),\n",
    "            'Page_Num': list(section_page_numbers)\n",
    "        })\n",
    "    \n",
    "    processed_data.append({\n",
    "        'file_name': file_name,\n",
    "        'sections': file_content\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hank/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/hank/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "embed_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "EMBEDDING_DIM = 1024  # **è«‹æ ¹æ“šä½ çš„ embedding æ¨¡å‹æ”¹è®Šé€™å€‹æ•¸å€¼**\n",
    "index = faiss.IndexFlatL2(EMBEDDING_DIM)  # L2 è·é›¢ç´¢å¼•\n",
    "faiss_metadata = {}\n",
    "\n",
    "\n",
    "for pdf_file in processed_data:\n",
    "    file_name = pdf_file['file_name']\n",
    "    for idx, chunk in enumerate(pdf_file['sections']):\n",
    "        text = chunk[\"content\"]\n",
    "\n",
    "        \n",
    "        # **ğŸš€ è¨ˆç®— embedding ä¸¦å­˜å…¥ cache**\n",
    "        embedding = embed_model.encode(text).astype(np.float32)\n",
    "\n",
    "        # **ğŸš€ å„²å­˜ metadata**\n",
    "        data.append({\n",
    "            \"File_Name\": file_name,\n",
    "            \"content\": text,\n",
    "            \"Page_Num\": ','.join(str(p) for p in chunk[\"Page_Num\"])\n",
    "        })\n",
    "        index.add(np.array([embedding]))  # åŠ å…¥ FAISS\n",
    "        faiss_metadata[len(faiss_metadata)] = data[-1]  # FAISS ID å°æ‡‰ metadata\n",
    "    \n",
    "faiss.write_index(index, \"faiss_element_chunk_index.idx\")\n",
    "with open(\"faiss_element_chunk_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(faiss_metadata, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
