{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF 文件萃取與向量化流程\n",
    "\n",
    "## 資料處理流程\n",
    "- 使用 OpenAI API 萃取 PDF 文件結構化內容\n",
    "- 將結構化內容轉換為向量表示\n",
    "- 使用 FAISS 建立向量資料庫儲存\n",
    "\n",
    "## 步驟\n",
    "1. PDF 內容萃取\n",
    "    - 使用 OpenAI API 進行文本結構化\n",
    "    - 將內容轉換為標準 JSON 格式\n",
    "2. 向量化處理\n",
    "    - 建立 FAISS 向量資料庫\n",
    "    - 將文件內容轉換為向量\n",
    "3. 資料儲存與索引\n",
    "    - 儲存向量索引\n",
    "    - 建立文件檢索機制\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  fitz\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    \"\"\"讀取 PDF 檔案並依據文本頁數返回其內容\"\"\"\n",
    "    page_content= []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        page_content.append({'page':page_num+1,'content':text})\n",
    "    doc.close()\n",
    "    return page_content\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "你是一個專業的文本標記助手，請根據以下文本內容，在適當的位置加上標記，並返回 **有效的 JSON 格式** 結果。\n",
    "### 文本內容：\n",
    "    {content}\n",
    "\n",
    "### **請使用以下標記類型**\n",
    "- `\"TITLE\"`：章節標題\n",
    "- `\"TEXT\"`：正文\n",
    "### **請確保**\n",
    "1. **保持原始文本內容**，僅添加標記，不要修改任何文字。  \n",
    "2. **每個段落作為一個獨立的區塊，標記為 `\"TITLE\"` 或 `\"TEXT\"`**。  \n",
    "3. **若內容為標題，標記為 `\"TITLE\"`，其餘內容標記為 `\"TEXT\"`**。  \n",
    "4. **`\"TEXT\"` 與 `\"TITLE\"` 內容需保持一致**，不可遺漏或刪減。  \n",
    "5. **輸出結果必須是 JSON 格式**，且符合以下結構：  \n",
    "\n",
    "```json\n",
    "{{\n",
    "    \"document\": [\n",
    "        {{\n",
    "            \"type\": \"TITLE\",\n",
    "            \"content\": \"標題文字\"\n",
    "        }},\n",
    "        {{\n",
    "            \"type\": \"TEXT\",\n",
    "            \"content\": \"正文內容...\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../pdf'\n",
    "question_list = []\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]\n",
    "\n",
    "api_key = \"\"\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file_name in pdf_files:\n",
    "    file_data = {\n",
    "        \"file_name\": file_name,\n",
    "        \"pages\": []\n",
    "    }\n",
    "    pdf_content = read_pdf(folder_path + '/' + file_name)\n",
    "    for page in pdf_content:\n",
    "        # Remove header text\n",
    "        page['content'] = page['content'].replace('臺灣學術網路危機處理中心(TACERT) \\nTANet Computer Emergency Response Team  ','')\n",
    "        \n",
    "        # Create prompt and get completion\n",
    "        prompt = prompt_template.format(content=page['content'])\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0.2,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Parse JSON response\n",
    "        json_str = completion.choices[0].message.content.strip().strip('```json\\n').strip('```')\n",
    "        parsed_json = json.loads(json_str)\n",
    "        \n",
    "        # Add page data\n",
    "        page_data = {\n",
    "            \"page_number\": page['page'],\n",
    "            \"content\": parsed_json\n",
    "        }\n",
    "        file_data[\"pages\"].append(page_data) \n",
    "    data.append(file_data)\n",
    "\n",
    "\n",
    "# Save data to JSON file with proper formatting\n",
    "with open('unstracted_pdf.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 向量化處理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unstracted_pdf.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "for file in data:\n",
    "    file_name = file['file_name']\n",
    "    current_title = ''\n",
    "    current_content = []\n",
    "    file_content = []\n",
    "    page_numbers = set()  # Using set to track unique page numbers\n",
    "    section_page_numbers = set()  # Track pages for current section\n",
    "\n",
    "    for page in file['pages']:\n",
    "        current_page = page['page_number']\n",
    "        \n",
    "        for item in page['content']['document']:\n",
    "            if item['type'] == 'TITLE':\n",
    "                # If we have a previous section, save it with its page numbers\n",
    "                if current_title and current_content:\n",
    "                    file_content.append({\n",
    "                        'content':  \"\".join(current_content),\n",
    "                        'Page_Num': list(section_page_numbers)  # Convert set to list\n",
    "                    })             \n",
    "                # Start new section\n",
    "                current_title = item['content']\n",
    "                current_content = [item['content']]\n",
    "                section_page_numbers = {current_page}  # Initialize with current page\n",
    "            else:\n",
    "                current_content.append(item['content'])\n",
    "                section_page_numbers.add(current_page)\n",
    "\n",
    "    # Don't forget to add the last section\n",
    "    if current_title and current_content:\n",
    "        file_content.append({\n",
    "            'content': \"\".join(current_content),\n",
    "            'Page_Num': list(section_page_numbers)\n",
    "        })\n",
    "    \n",
    "    processed_data.append({\n",
    "        'file_name': file_name,\n",
    "        'sections': file_content\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hank/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/hank/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "# 初始化嵌入模型\n",
    "embed_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "EMBEDDING_DIM = 1024  # **請根據你的 embedding 模型改變這個數值**\n",
    "index = faiss.IndexFlatL2(EMBEDDING_DIM)  # L2 距離索引\n",
    "faiss_metadata = {}\n",
    "\n",
    "\n",
    "for pdf_file in processed_data:\n",
    "    file_name = pdf_file['file_name']\n",
    "    for idx, chunk in enumerate(pdf_file['sections']):\n",
    "        text = chunk[\"content\"]\n",
    "\n",
    "        \n",
    "        # **🚀 計算 embedding 並存入 cache**\n",
    "        embedding = embed_model.encode(text).astype(np.float32)\n",
    "\n",
    "        # **🚀 儲存 metadata**\n",
    "        data.append({\n",
    "            \"File_Name\": file_name,\n",
    "            \"content\": text,\n",
    "            \"Page_Num\": ','.join(str(p) for p in chunk[\"Page_Num\"])\n",
    "        })\n",
    "        index.add(np.array([embedding]))  # 加入 FAISS\n",
    "        faiss_metadata[len(faiss_metadata)] = data[-1]  # FAISS ID 對應 metadata\n",
    "    \n",
    "faiss.write_index(index, \"faiss_element_chunk_index.idx\")\n",
    "with open(\"faiss_element_chunk_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(faiss_metadata, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
