{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import fitz\n",
    "import os\n",
    "from chromadb import PersistentClient\n",
    "# 初始化嵌入模型\n",
    "embed_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "local_chunk_set = []\n",
    "\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    \"\"\"讀取 PDF 檔案並依據文本頁數返回其內容\"\"\"\n",
    "    page_content= []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        page_content.append({'page':page_num+1,'content':text})\n",
    "    doc.close()\n",
    "    return page_content\n",
    "\n",
    "\n",
    "def chunk_text(pages, chunk_size):\n",
    "    \"\"\"將文本拆分為指定大小的 chunks，允許 chunk 跨越頁數，並標記 chunk 涉及的頁數範圍\"\"\"\n",
    "    chunks = []\n",
    "    all_text = \"\"\n",
    "    page_mapping = []\n",
    "    \n",
    "    for page in pages:\n",
    "        start_idx = len(all_text)\n",
    "        all_text += page['content'] + \"\\n\"\n",
    "        end_idx = len(all_text)\n",
    "        page_mapping.append((start_idx, end_idx, page['page']))\n",
    "    \n",
    "    tokens = embed_model.tokenizer.tokenize(all_text)\n",
    "    \n",
    "    for i in range(0, len(tokens), chunk_size):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunk_text = embed_model.tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        \n",
    "        chunk_pages = set()\n",
    "        chunk_start = len(embed_model.tokenizer.convert_tokens_to_string(tokens[:i]))\n",
    "        chunk_end = len(embed_model.tokenizer.convert_tokens_to_string(tokens[:i + chunk_size]))\n",
    "        \n",
    "        for start_idx, end_idx, page in page_mapping:\n",
    "            if chunk_start < end_idx and chunk_end > start_idx:\n",
    "                chunk_pages.add(page)\n",
    "        \n",
    "        chunks.append({'pages': sorted(chunk_pages), 'content': chunk_text})\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../pdf'\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始的chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已儲存 192 筆資料到 FAISS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# 儲存 embeddings 避免重複計算\n",
    "embedding_cache = {}\n",
    "data = []\n",
    "\n",
    "# FAISS 索引初始化（假設 embedding 維度為 384）\n",
    "EMBEDDING_DIM = 1024  # **請根據你的 embedding 模型改變這個數值**\n",
    "index = faiss.IndexFlatL2(EMBEDDING_DIM)  # L2 距離索引\n",
    "\n",
    "# 儲存 FAISS ID 對應的 metadata\n",
    "faiss_metadata = {}\n",
    "\n",
    "# 設定批次大小，避免 OOM\n",
    "\n",
    "chunk_sizes = [128,256,512,1024]  # 測試不同 chunk sizes\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    full_path = os.path.join(folder_path, pdf_file)\n",
    "    content = read_pdf(full_path)\n",
    "\n",
    "    for size in chunk_sizes:\n",
    "        chunks = chunk_text(content, size)\n",
    "\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            text = chunk[\"content\"]\n",
    "\n",
    "            # **🚀 檢查 embedding 是否已經計算過**\n",
    "            if text in embedding_cache:\n",
    "                continue  # 跳過已處理過的文本\n",
    "            \n",
    "            # **🚀 計算 embedding 並存入 cache**\n",
    "            embedding = embed_model.encode(text).astype(np.float32)\n",
    "            embedding_cache[text] = embedding\n",
    "\n",
    "            # **🚀 儲存 metadata**\n",
    "            data.append({\n",
    "                \"File_Name\": pdf_file,\n",
    "                \"content\": text,\n",
    "                \"Page_Num\": ','.join(str(p) for p in chunk[\"pages\"])\n",
    "            })\n",
    "\n",
    "            # **🚀 將 embedding 加入 FAISS**\n",
    "            index.add(np.array([embedding]))  # 加入 FAISS\n",
    "            faiss_metadata[len(faiss_metadata)] = data[-1]  # FAISS ID 對應 metadata\n",
    "\n",
    "# **🚀 儲存 FAISS 索引**\n",
    "faiss.write_index(index, \"faiss_index.idx\")\n",
    "with open(\"faiss_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(faiss_metadata, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"已儲存 {len(faiss_metadata)} 筆資料到 FAISS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary_keyword_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算 naive chunk 的準確度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9145299145299145\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = faiss.read_index(\"faiss_index.idx\")\n",
    "with open(\"faiss_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    faiss_metadata = json.load(f)\n",
    "def search_faiss(query_text, top_k=3):\n",
    "    \"\"\"搜尋最相近的前 top_k 筆資料\"\"\"\n",
    "    query_embedding = embed_model.encode(query_text).astype(np.float32)\n",
    "    query_embedding = np.expand_dims(query_embedding, axis=0)  # FAISS 需要 2D 陣列\n",
    "\n",
    "    # **🚀 查詢 FAISS**\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        if str(idx) in faiss_metadata:\n",
    "            results.append(faiss_metadata[str(idx)])\n",
    "\n",
    "    return results\n",
    "\n",
    "with open('pdf_questions.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "win = 0\n",
    "loss = 0\n",
    "for each in data : \n",
    "\n",
    "\n",
    "    res = search_faiss(each['question'])\n",
    "    found = False\n",
    "    for metadata in res:\n",
    "        if metadata['File_Name'] == each['File_Name'] and str(each['Page_Num']) in  metadata['Page_Num']:\n",
    "            win += 1\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        loss += 1\n",
    "\n",
    "print(win/(win+loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
