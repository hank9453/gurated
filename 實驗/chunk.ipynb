{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import fitz\n",
    "import os\n",
    "from chromadb import PersistentClient\n",
    "# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "embed_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "local_chunk_set = []\n",
    "\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    \"\"\"è®€å– PDF æª”æ¡ˆä¸¦ä¾æ“šæ–‡æœ¬é æ•¸è¿”å›å…¶å…§å®¹\"\"\"\n",
    "    page_content= []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        page_content.append({'page':page_num+1,'content':text})\n",
    "    doc.close()\n",
    "    return page_content\n",
    "\n",
    "\n",
    "def chunk_text(pages, chunk_size):\n",
    "    \"\"\"å°‡æ–‡æœ¬æ‹†åˆ†ç‚ºæŒ‡å®šå¤§å°çš„ chunksï¼Œå…è¨± chunk è·¨è¶Šé æ•¸ï¼Œä¸¦æ¨™è¨˜ chunk æ¶‰åŠçš„é æ•¸ç¯„åœ\"\"\"\n",
    "    chunks = []\n",
    "    all_text = \"\"\n",
    "    page_mapping = []\n",
    "    \n",
    "    for page in pages:\n",
    "        start_idx = len(all_text)\n",
    "        all_text += page['content'] + \"\\n\"\n",
    "        end_idx = len(all_text)\n",
    "        page_mapping.append((start_idx, end_idx, page['page']))\n",
    "    \n",
    "    tokens = embed_model.tokenizer.tokenize(all_text)\n",
    "    \n",
    "    for i in range(0, len(tokens), chunk_size):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunk_text = embed_model.tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        \n",
    "        chunk_pages = set()\n",
    "        chunk_start = len(embed_model.tokenizer.convert_tokens_to_string(tokens[:i]))\n",
    "        chunk_end = len(embed_model.tokenizer.convert_tokens_to_string(tokens[:i + chunk_size]))\n",
    "        \n",
    "        for start_idx, end_idx, page in page_mapping:\n",
    "            if chunk_start < end_idx and chunk_end > start_idx:\n",
    "                chunk_pages.add(page)\n",
    "        \n",
    "        chunks.append({'pages': sorted(chunk_pages), 'content': chunk_text})\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../pdf'\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŸå§‹çš„chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å„²å­˜ 192 ç­†è³‡æ–™åˆ° FAISS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# å„²å­˜ embeddings é¿å…é‡è¤‡è¨ˆç®—\n",
    "embedding_cache = {}\n",
    "data = []\n",
    "\n",
    "# FAISS ç´¢å¼•åˆå§‹åŒ–ï¼ˆå‡è¨­ embedding ç¶­åº¦ç‚º 384ï¼‰\n",
    "EMBEDDING_DIM = 1024  # **è«‹æ ¹æ“šä½ çš„ embedding æ¨¡å‹æ”¹è®Šé€™å€‹æ•¸å€¼**\n",
    "index = faiss.IndexFlatL2(EMBEDDING_DIM)  # L2 è·é›¢ç´¢å¼•\n",
    "\n",
    "# å„²å­˜ FAISS ID å°æ‡‰çš„ metadata\n",
    "faiss_metadata = {}\n",
    "\n",
    "# è¨­å®šæ‰¹æ¬¡å¤§å°ï¼Œé¿å… OOM\n",
    "\n",
    "chunk_sizes = [128,256,512,1024]  # æ¸¬è©¦ä¸åŒ chunk sizes\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    full_path = os.path.join(folder_path, pdf_file)\n",
    "    content = read_pdf(full_path)\n",
    "\n",
    "    for size in chunk_sizes:\n",
    "        chunks = chunk_text(content, size)\n",
    "\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            text = chunk[\"content\"]\n",
    "\n",
    "            # **ğŸš€ æª¢æŸ¥ embedding æ˜¯å¦å·²ç¶“è¨ˆç®—é**\n",
    "            if text in embedding_cache:\n",
    "                continue  # è·³éå·²è™•ç†éçš„æ–‡æœ¬\n",
    "            \n",
    "            # **ğŸš€ è¨ˆç®— embedding ä¸¦å­˜å…¥ cache**\n",
    "            embedding = embed_model.encode(text).astype(np.float32)\n",
    "            embedding_cache[text] = embedding\n",
    "\n",
    "            # **ğŸš€ å„²å­˜ metadata**\n",
    "            data.append({\n",
    "                \"File_Name\": pdf_file,\n",
    "                \"content\": text,\n",
    "                \"Page_Num\": ','.join(str(p) for p in chunk[\"pages\"])\n",
    "            })\n",
    "\n",
    "            # **ğŸš€ å°‡ embedding åŠ å…¥ FAISS**\n",
    "            index.add(np.array([embedding]))  # åŠ å…¥ FAISS\n",
    "            faiss_metadata[len(faiss_metadata)] = data[-1]  # FAISS ID å°æ‡‰ metadata\n",
    "\n",
    "# **ğŸš€ å„²å­˜ FAISS ç´¢å¼•**\n",
    "faiss.write_index(index, \"faiss_index.idx\")\n",
    "with open(\"faiss_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(faiss_metadata, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"å·²å„²å­˜ {len(faiss_metadata)} ç­†è³‡æ–™åˆ° FAISS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary_keyword_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¨ˆç®— naive chunk çš„æº–ç¢ºåº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9145299145299145\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = faiss.read_index(\"faiss_index.idx\")\n",
    "with open(\"faiss_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    faiss_metadata = json.load(f)\n",
    "def search_faiss(query_text, top_k=3):\n",
    "    \"\"\"æœå°‹æœ€ç›¸è¿‘çš„å‰ top_k ç­†è³‡æ–™\"\"\"\n",
    "    query_embedding = embed_model.encode(query_text).astype(np.float32)\n",
    "    query_embedding = np.expand_dims(query_embedding, axis=0)  # FAISS éœ€è¦ 2D é™£åˆ—\n",
    "\n",
    "    # **ğŸš€ æŸ¥è©¢ FAISS**\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        if str(idx) in faiss_metadata:\n",
    "            results.append(faiss_metadata[str(idx)])\n",
    "\n",
    "    return results\n",
    "\n",
    "with open('pdf_questions.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "win = 0\n",
    "loss = 0\n",
    "for each in data : \n",
    "\n",
    "\n",
    "    res = search_faiss(each['question'])\n",
    "    found = False\n",
    "    for metadata in res:\n",
    "        if metadata['File_Name'] == each['File_Name'] and str(each['Page_Num']) in  metadata['Page_Num']:\n",
    "            win += 1\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        loss += 1\n",
    "\n",
    "print(win/(win+loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
