{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import fitz\n",
    "import os\n",
    "\n",
    "# 初始化嵌入模型\n",
    "embed_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "local_chunk_set = []\n",
    "\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    \"\"\"讀取 PDF 檔案並依據文本頁數返回其內容\"\"\"\n",
    "    page_content= []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        page_content.append({'page':page_num+1,'content':text})\n",
    "    doc.close()\n",
    "    return page_content\n",
    "\n",
    "\n",
    "def chunk_text(pages, chunk_size):\n",
    "    \"\"\"將文本拆分為指定大小的 chunks，允許 chunk 跨越頁數，並標記 chunk 涉及的頁數範圍\"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_pages = set()\n",
    "    current_length = 0\n",
    "    max_token_length = chunk_size  \n",
    "\n",
    "    for page in pages:\n",
    "        text = page['content']\n",
    "        text_tokens = embed_model.tokenizer.tokenize(text)\n",
    "\n",
    "        while text_tokens:\n",
    "            space_left = max_token_length - current_length\n",
    "\n",
    "            # 如果當前 chunk 還有空間\n",
    "            if space_left > 0:\n",
    "                tokens_to_add = text_tokens[:space_left]\n",
    "                text_tokens = text_tokens[space_left:]\n",
    "\n",
    "                current_chunk.extend(tokens_to_add)\n",
    "                current_pages.add(page['page'])\n",
    "                current_length += len(tokens_to_add)\n",
    "\n",
    "            # 當 chunk 滿了，就存入 chunks，並重置變數\n",
    "            if current_length >= max_token_length or (current_length > 0 and len(text_tokens) > 0):\n",
    "                chunks.append({\n",
    "                    'pages': sorted(current_pages),\n",
    "                    'content': embed_model.tokenizer.convert_tokens_to_string(current_chunk)\n",
    "                })\n",
    "                current_chunk = []\n",
    "                current_pages = set()\n",
    "                current_length = 0\n",
    "\n",
    "    # 處理最後一個未滿的 chunk\n",
    "    if current_length > 0:\n",
    "        chunks.append({\n",
    "            'pages': sorted(current_pages),\n",
    "            'content': embed_model.tokenizer.convert_tokens_to_string(current_chunk)\n",
    "        })\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../pdf'\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始的chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已儲存 230 筆資料到 FAISS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing p: 100%|██████████| 105/105 [00:01<00:00, 70.99it/s]\n",
      "Featurizing q: 100%|██████████| 155/155 [00:02<00:00, 73.06it/s]\n",
      "Featurizing p: 100%|██████████| 38/38 [00:00<00:00, 63.26it/s]\n",
      "Featurizing q: 100%|██████████| 36/36 [00:00<00:00, 73.93it/s]\n",
      "Featurizing p: 100%|██████████| 57/57 [00:00<00:00, 74.80it/s]\n",
      "Featurizing q:  60%|██████    | 79/131 [00:01<00:00, 74.91it/s]"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from mauve import compute_mauve\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "def calculate_rouge_score(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    return scores['rougeL'].fmeasure  # 取 ROUGE-L 分數\n",
    "def search_faiss(query_text, top_k=3):\n",
    "    \"\"\"搜尋最相近的前 top_k 筆資料\"\"\"\n",
    "    query_embedding = embed_model.encode(query_text).astype(np.float32)\n",
    "    query_embedding = np.expand_dims(query_embedding, axis=0)  # FAISS 需要 2D 陣列\n",
    "\n",
    "    # **🚀 查詢 FAISS**\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        if str(idx) in faiss_metadata:\n",
    "            results.append(faiss_metadata[str(idx)])\n",
    "\n",
    "    return results\n",
    "def calculate_mauve_score(reference, candidate):\n",
    "    return compute_mauve(p_text=reference, q_text=candidate, verbose=False,device_id=0).mauve\n",
    "llm = Ollama(model=\"jcai/llama-3-taiwan-8b-instruct:q4_k_m\", temperature=0.2)\n",
    "\n",
    "\n",
    "# 儲存 FAISS ID 對應的 metadata\n",
    "\n",
    "# 設定批次大小，避免 OOM\n",
    "result = []\n",
    "chunk_sizes = [64,128, 256, 512, 1024]  # 測試不同 chunk sizes\n",
    "for size in chunk_sizes:\n",
    "    EMBEDDING_DIM = 1024  \n",
    "    index = faiss.IndexFlatL2(EMBEDDING_DIM)  # L2 距離索引\n",
    "    data = []\n",
    "    faiss_metadata = {}\n",
    "    files_to_delete = ['faiss_metadata.json', 'faiss_index.idx']\n",
    "    for file in files_to_delete:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "    for pdf_file in pdf_files:\n",
    "        full_path = os.path.join(folder_path, pdf_file)\n",
    "        content = read_pdf(full_path)\n",
    "        chunks = chunk_text(content, size)\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            text = chunk[\"content\"]\n",
    "            # **計算 embedding 並存入 cache**\n",
    "            embedding = embed_model.encode(text).astype(np.float32)\n",
    "\n",
    "            # ** 儲存 metadata**\n",
    "            data.append({\n",
    "                \"File_Name\": pdf_file,\n",
    "                \"content\": text,\n",
    "                \"Page_Num\": ','.join(str(p) for p in chunk[\"pages\"])\n",
    "            })\n",
    "            index.add(np.array([embedding]))  # 加入 FAISS\n",
    "            faiss_metadata[len(faiss_metadata)] = data[-1]  # FAISS ID 對應 metadata\n",
    "    # ** 儲存 FAISS 索引**\n",
    "    faiss.write_index(index, \"faiss_index.idx\")\n",
    "    with open(\"faiss_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(faiss_metadata, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"已儲存 {len(faiss_metadata)} 筆資料到 FAISS\")\n",
    "    index = faiss.IndexFlatL2(EMBEDDING_DIM)\n",
    "    index = faiss.read_index(\"faiss_index.idx\")\n",
    "    with open(\"faiss_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        faiss_metadata = json.load(f)\n",
    "    with open('pdf_questions.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    win = 0\n",
    "    loss = 0\n",
    "    total_rouge = 0\n",
    "    total_mauve = 0\n",
    "            \n",
    "    for each in data:\n",
    "        res = search_faiss(each['question'])\n",
    "        found = False\n",
    "        \n",
    "        # 整理 Context\n",
    "        context = \" \".join([metadata['content'] for metadata in res])\n",
    "        \n",
    "        # LLM Prompt\n",
    "        prompt = f\"\"\"\n",
    "        你是一個 AI 助理，專門根據官方文件回答資安相關問題。請根據提供的文件內容生成精確且簡潔的答案，不要加入額外資訊或推測性的內容，並確保答案與參考內容高度一致。\n",
    "\n",
    "        **問題：** {each['question']}\n",
    "        **參考內容：** {context}\n",
    "\n",
    "        **答案（請直接從參考內容提取，確保準確性以及符合問題）：**\n",
    "        \"\"\"\n",
    "        \n",
    "        # 生成回答\n",
    "        generated_answer = llm.invoke(prompt).strip()\n",
    "        \n",
    "        # 計算 ROUGE 分數\n",
    "        rouge_score = calculate_rouge_score(each['answer'], generated_answer)\n",
    "        total_rouge += rouge_score\n",
    "        \n",
    "        # 計算 MAUVE 分數\n",
    "        try : \n",
    "            mauve_score = calculate_mauve_score(each['answer'], generated_answer)\n",
    "            total_mauve += mauve_score\n",
    "        except : \n",
    "            pass\n",
    "        # 確認是否匹配文件名稱和頁碼\n",
    "        for metadata in res:\n",
    "            if metadata['File_Name'] == each['File_Name'] and str(each['Page_Num']) in metadata['Page_Num']:\n",
    "                win += 1\n",
    "                found = True\n",
    "                break  # 找到匹配的就不繼續檢查\n",
    "        if not found:\n",
    "            loss += 1\n",
    "        \n",
    "    accuracy = win / (win + loss) if (win + loss) > 0 else 0\n",
    "    result.append({'chunk_size': size, 'total_chunk': len(faiss_metadata), 'accuracy': accuracy, 'rougeL': total_rouge / (win + loss), 'mauve': total_mauve / (win + loss)})\n",
    "\n",
    "for each in result:\n",
    "    print(each)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算 element chunk 的準確度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_mauve() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m rough_score \u001b[38;5;241m=\u001b[39m calculate_rouge_score(each[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m], generated_answer)\n\u001b[0;32m     41\u001b[0m total_rough \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rough_score\n\u001b[1;32m---> 42\u001b[0m mauve_score \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_mauve_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43meach\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_answer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m total_mauve \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mauve_score\n\u001b[0;32m     44\u001b[0m res \u001b[38;5;241m=\u001b[39m search_faiss(each[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[38], line 8\u001b[0m, in \u001b[0;36mcalculate_mauve_score\u001b[1;34m(reference, candidate)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_mauve_score\u001b[39m(reference, candidate):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# 確保 torch 使用 CUDA\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute_mauve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcandidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 指定運行設備\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmauve\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_mauve() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "\n",
    "index = faiss.IndexFlatL2(EMBEDDING_DIM)\n",
    "index = faiss.read_index(\"faiss_element_chunk_index.idx\")\n",
    "with open(\"faiss_element_chunk_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    faiss_metadata = json.load(f)\n",
    "with open('pdf_questions.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "total_mauve = 0\n",
    "total_rough=0\n",
    "win = 0\n",
    "loss = 0\n",
    "for each in data : \n",
    "        # 整理 Context\n",
    "    context = \" \".join([metadata['content'] for metadata in res])\n",
    "    # LLM Prompt\n",
    "    prompt = f\"\"\"\n",
    "    你是一個 AI 助理，專門根據官方文件回答資安相關問題。請根據提供的文件內容生成精確且簡潔的答案，不要加入額外資訊或推測性的內容，並確保答案與參考內容高度一致。\n",
    "\n",
    "    **問題：** {each['question']}\n",
    "    **參考內容：** {context}\n",
    "\n",
    "    **答案（請直接從參考內容提取，確保準確性以及符合問題）：**\n",
    "    \"\"\"\n",
    "        # 生成回答\n",
    "    generated_answer = llm.invoke(prompt).strip()\n",
    "    # 計算 BLEU 和 ROUGE 分數\n",
    "    rough_score = calculate_rouge_score(each['answer'], generated_answer)\n",
    "    total_rough += rough_score\n",
    "    mauve_score = calculate_mauve_score(each['answer'], generated_answer)\n",
    "    total_mauve += mauve_score\n",
    "    res = search_faiss(each['question'])\n",
    "    found = False\n",
    "    for metadata in res:\n",
    "        if metadata['File_Name'] == each['File_Name'] and str(each['Page_Num']) in  metadata['Page_Num']:\n",
    "            win += 1\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        loss += 1\n",
    "\n",
    "accuracy = win / (win + loss) if (win + loss) > 0 else 0\n",
    "average_rough = total_rough / (win + loss) \n",
    "average_mauve = total_mauve / (win + loss)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Average mauve score: {average_mauve:.4f}\")\n",
    "print(f\"Average ROUGE-L score: {average_rough:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
