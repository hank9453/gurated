{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import fitz\n",
    "import os\n",
    "\n",
    "# åˆå§‹åŒ–åµŒå…¥æ¨¡åž‹\n",
    "embed_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "local_chunk_set = []\n",
    "\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    \"\"\"è®€å– PDF æª”æ¡ˆä¸¦ä¾æ“šæ–‡æœ¬é æ•¸è¿”å›žå…¶å…§å®¹\"\"\"\n",
    "    page_content= []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        page_content.append({'page':page_num+1,'content':text})\n",
    "    doc.close()\n",
    "    return page_content\n",
    "\n",
    "\n",
    "def chunk_text(pages, chunk_size):\n",
    "    \"\"\"å°‡æ–‡æœ¬æ‹†åˆ†ç‚ºæŒ‡å®šå¤§å°çš„ chunksï¼Œå…è¨± chunk è·¨è¶Šé æ•¸ï¼Œä¸¦æ¨™è¨˜ chunk æ¶‰åŠçš„é æ•¸ç¯„åœ\"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_pages = set()\n",
    "    current_length = 0\n",
    "    max_token_length = chunk_size  \n",
    "\n",
    "    for page in pages:\n",
    "        text = page['content']\n",
    "        text_tokens = embed_model.tokenizer.tokenize(text)\n",
    "\n",
    "        while text_tokens:\n",
    "            space_left = max_token_length - current_length\n",
    "\n",
    "            # å¦‚æžœç•¶å‰ chunk é‚„æœ‰ç©ºé–“\n",
    "            if space_left > 0:\n",
    "                tokens_to_add = text_tokens[:space_left]\n",
    "                text_tokens = text_tokens[space_left:]\n",
    "\n",
    "                current_chunk.extend(tokens_to_add)\n",
    "                current_pages.add(page['page'])\n",
    "                current_length += len(tokens_to_add)\n",
    "\n",
    "            # ç•¶ chunk æ»¿äº†ï¼Œå°±å­˜å…¥ chunksï¼Œä¸¦é‡ç½®è®Šæ•¸\n",
    "            if current_length >= max_token_length or (current_length > 0 and len(text_tokens) > 0):\n",
    "                chunks.append({\n",
    "                    'pages': sorted(current_pages),\n",
    "                    'content': embed_model.tokenizer.convert_tokens_to_string(current_chunk)\n",
    "                })\n",
    "                current_chunk = []\n",
    "                current_pages = set()\n",
    "                current_length = 0\n",
    "\n",
    "    # è™•ç†æœ€å¾Œä¸€å€‹æœªæ»¿çš„ chunk\n",
    "    if current_length > 0:\n",
    "        chunks.append({\n",
    "            'pages': sorted(current_pages),\n",
    "            'content': embed_model.tokenizer.convert_tokens_to_string(current_chunk)\n",
    "        })\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../pdf'\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŽŸå§‹çš„chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å„²å­˜ 230 ç­†è³‡æ–™åˆ° FAISS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing p: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 70.99it/s]\n",
      "Featurizing q: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:02<00:00, 73.06it/s]\n",
      "Featurizing p: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:00<00:00, 63.26it/s]\n",
      "Featurizing q: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:00<00:00, 73.93it/s]\n",
      "Featurizing p: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 74.80it/s]\n",
      "Featurizing q:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 79/131 [00:01<00:00, 74.91it/s]"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from mauve import compute_mauve\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "def calculate_rouge_score(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    return scores['rougeL'].fmeasure  # å– ROUGE-L åˆ†æ•¸\n",
    "def search_faiss(query_text, top_k=3):\n",
    "    \"\"\"æœå°‹æœ€ç›¸è¿‘çš„å‰ top_k ç­†è³‡æ–™\"\"\"\n",
    "    query_embedding = embed_model.encode(query_text).astype(np.float32)\n",
    "    query_embedding = np.expand_dims(query_embedding, axis=0)  # FAISS éœ€è¦ 2D é™£åˆ—\n",
    "\n",
    "    # **ðŸš€ æŸ¥è©¢ FAISS**\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        if str(idx) in faiss_metadata:\n",
    "            results.append(faiss_metadata[str(idx)])\n",
    "\n",
    "    return results\n",
    "def calculate_mauve_score(reference, candidate):\n",
    "    return compute_mauve(p_text=reference, q_text=candidate, verbose=False,device_id=0).mauve\n",
    "llm = Ollama(model=\"jcai/llama-3-taiwan-8b-instruct:q4_k_m\", temperature=0.2)\n",
    "\n",
    "\n",
    "# å„²å­˜ FAISS ID å°æ‡‰çš„ metadata\n",
    "\n",
    "# è¨­å®šæ‰¹æ¬¡å¤§å°ï¼Œé¿å… OOM\n",
    "result = []\n",
    "chunk_sizes = [64,128, 256, 512, 1024]  # æ¸¬è©¦ä¸åŒ chunk sizes\n",
    "for size in chunk_sizes:\n",
    "    EMBEDDING_DIM = 1024  \n",
    "    index = faiss.IndexFlatL2(EMBEDDING_DIM)  # L2 è·é›¢ç´¢å¼•\n",
    "    data = []\n",
    "    faiss_metadata = {}\n",
    "    files_to_delete = ['faiss_metadata.json', 'faiss_index.idx']\n",
    "    for file in files_to_delete:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "    for pdf_file in pdf_files:\n",
    "        full_path = os.path.join(folder_path, pdf_file)\n",
    "        content = read_pdf(full_path)\n",
    "        chunks = chunk_text(content, size)\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            text = chunk[\"content\"]\n",
    "            # **è¨ˆç®— embedding ä¸¦å­˜å…¥ cache**\n",
    "            embedding = embed_model.encode(text).astype(np.float32)\n",
    "\n",
    "            # ** å„²å­˜ metadata**\n",
    "            data.append({\n",
    "                \"File_Name\": pdf_file,\n",
    "                \"content\": text,\n",
    "                \"Page_Num\": ','.join(str(p) for p in chunk[\"pages\"])\n",
    "            })\n",
    "            index.add(np.array([embedding]))  # åŠ å…¥ FAISS\n",
    "            faiss_metadata[len(faiss_metadata)] = data[-1]  # FAISS ID å°æ‡‰ metadata\n",
    "    # ** å„²å­˜ FAISS ç´¢å¼•**\n",
    "    faiss.write_index(index, \"faiss_index.idx\")\n",
    "    with open(\"faiss_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(faiss_metadata, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"å·²å„²å­˜ {len(faiss_metadata)} ç­†è³‡æ–™åˆ° FAISS\")\n",
    "    index = faiss.IndexFlatL2(EMBEDDING_DIM)\n",
    "    index = faiss.read_index(\"faiss_index.idx\")\n",
    "    with open(\"faiss_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        faiss_metadata = json.load(f)\n",
    "    with open('pdf_questions.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    win = 0\n",
    "    loss = 0\n",
    "    total_rouge = 0\n",
    "    total_mauve = 0\n",
    "            \n",
    "    for each in data:\n",
    "        res = search_faiss(each['question'])\n",
    "        found = False\n",
    "        \n",
    "        # æ•´ç† Context\n",
    "        context = \" \".join([metadata['content'] for metadata in res])\n",
    "        \n",
    "        # LLM Prompt\n",
    "        prompt = f\"\"\"\n",
    "        ä½ æ˜¯ä¸€å€‹ AI åŠ©ç†ï¼Œå°ˆé–€æ ¹æ“šå®˜æ–¹æ–‡ä»¶å›žç­”è³‡å®‰ç›¸é—œå•é¡Œã€‚è«‹æ ¹æ“šæä¾›çš„æ–‡ä»¶å…§å®¹ç”Ÿæˆç²¾ç¢ºä¸”ç°¡æ½”çš„ç­”æ¡ˆï¼Œä¸è¦åŠ å…¥é¡å¤–è³‡è¨Šæˆ–æŽ¨æ¸¬æ€§çš„å…§å®¹ï¼Œä¸¦ç¢ºä¿ç­”æ¡ˆèˆ‡åƒè€ƒå…§å®¹é«˜åº¦ä¸€è‡´ã€‚\n",
    "\n",
    "        **å•é¡Œï¼š** {each['question']}\n",
    "        **åƒè€ƒå…§å®¹ï¼š** {context}\n",
    "\n",
    "        **ç­”æ¡ˆï¼ˆè«‹ç›´æŽ¥å¾žåƒè€ƒå…§å®¹æå–ï¼Œç¢ºä¿æº–ç¢ºæ€§ä»¥åŠç¬¦åˆå•é¡Œï¼‰ï¼š**\n",
    "        \"\"\"\n",
    "        \n",
    "        # ç”Ÿæˆå›žç­”\n",
    "        generated_answer = llm.invoke(prompt).strip()\n",
    "        \n",
    "        # è¨ˆç®— ROUGE åˆ†æ•¸\n",
    "        rouge_score = calculate_rouge_score(each['answer'], generated_answer)\n",
    "        total_rouge += rouge_score\n",
    "        \n",
    "        # è¨ˆç®— MAUVE åˆ†æ•¸\n",
    "        try : \n",
    "            mauve_score = calculate_mauve_score(each['answer'], generated_answer)\n",
    "            total_mauve += mauve_score\n",
    "        except : \n",
    "            pass\n",
    "        # ç¢ºèªæ˜¯å¦åŒ¹é…æ–‡ä»¶åç¨±å’Œé ç¢¼\n",
    "        for metadata in res:\n",
    "            if metadata['File_Name'] == each['File_Name'] and str(each['Page_Num']) in metadata['Page_Num']:\n",
    "                win += 1\n",
    "                found = True\n",
    "                break  # æ‰¾åˆ°åŒ¹é…çš„å°±ä¸ç¹¼çºŒæª¢æŸ¥\n",
    "        if not found:\n",
    "            loss += 1\n",
    "        \n",
    "    accuracy = win / (win + loss) if (win + loss) > 0 else 0\n",
    "    result.append({'chunk_size': size, 'total_chunk': len(faiss_metadata), 'accuracy': accuracy, 'rougeL': total_rouge / (win + loss), 'mauve': total_mauve / (win + loss)})\n",
    "\n",
    "for each in result:\n",
    "    print(each)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¨ˆç®— element chunk çš„æº–ç¢ºåº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_mauve() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m rough_score \u001b[38;5;241m=\u001b[39m calculate_rouge_score(each[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m], generated_answer)\n\u001b[0;32m     41\u001b[0m total_rough \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rough_score\n\u001b[1;32m---> 42\u001b[0m mauve_score \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_mauve_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43meach\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_answer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m total_mauve \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mauve_score\n\u001b[0;32m     44\u001b[0m res \u001b[38;5;241m=\u001b[39m search_faiss(each[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[38], line 8\u001b[0m, in \u001b[0;36mcalculate_mauve_score\u001b[1;34m(reference, candidate)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_mauve_score\u001b[39m(reference, candidate):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# ç¢ºä¿ torch ä½¿ç”¨ CUDA\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute_mauve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcandidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# æŒ‡å®šé‹è¡Œè¨­å‚™\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmauve\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_mauve() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "\n",
    "index = faiss.IndexFlatL2(EMBEDDING_DIM)\n",
    "index = faiss.read_index(\"faiss_element_chunk_index.idx\")\n",
    "with open(\"faiss_element_chunk_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    faiss_metadata = json.load(f)\n",
    "with open('pdf_questions.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "total_mauve = 0\n",
    "total_rough=0\n",
    "win = 0\n",
    "loss = 0\n",
    "for each in data : \n",
    "        # æ•´ç† Context\n",
    "    context = \" \".join([metadata['content'] for metadata in res])\n",
    "    # LLM Prompt\n",
    "    prompt = f\"\"\"\n",
    "    ä½ æ˜¯ä¸€å€‹ AI åŠ©ç†ï¼Œå°ˆé–€æ ¹æ“šå®˜æ–¹æ–‡ä»¶å›žç­”è³‡å®‰ç›¸é—œå•é¡Œã€‚è«‹æ ¹æ“šæä¾›çš„æ–‡ä»¶å…§å®¹ç”Ÿæˆç²¾ç¢ºä¸”ç°¡æ½”çš„ç­”æ¡ˆï¼Œä¸è¦åŠ å…¥é¡å¤–è³‡è¨Šæˆ–æŽ¨æ¸¬æ€§çš„å…§å®¹ï¼Œä¸¦ç¢ºä¿ç­”æ¡ˆèˆ‡åƒè€ƒå…§å®¹é«˜åº¦ä¸€è‡´ã€‚\n",
    "\n",
    "    **å•é¡Œï¼š** {each['question']}\n",
    "    **åƒè€ƒå…§å®¹ï¼š** {context}\n",
    "\n",
    "    **ç­”æ¡ˆï¼ˆè«‹ç›´æŽ¥å¾žåƒè€ƒå…§å®¹æå–ï¼Œç¢ºä¿æº–ç¢ºæ€§ä»¥åŠç¬¦åˆå•é¡Œï¼‰ï¼š**\n",
    "    \"\"\"\n",
    "        # ç”Ÿæˆå›žç­”\n",
    "    generated_answer = llm.invoke(prompt).strip()\n",
    "    # è¨ˆç®— BLEU å’Œ ROUGE åˆ†æ•¸\n",
    "    rough_score = calculate_rouge_score(each['answer'], generated_answer)\n",
    "    total_rough += rough_score\n",
    "    mauve_score = calculate_mauve_score(each['answer'], generated_answer)\n",
    "    total_mauve += mauve_score\n",
    "    res = search_faiss(each['question'])\n",
    "    found = False\n",
    "    for metadata in res:\n",
    "        if metadata['File_Name'] == each['File_Name'] and str(each['Page_Num']) in  metadata['Page_Num']:\n",
    "            win += 1\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        loss += 1\n",
    "\n",
    "accuracy = win / (win + loss) if (win + loss) > 0 else 0\n",
    "average_rough = total_rough / (win + loss) \n",
    "average_mauve = total_mauve / (win + loss)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Average mauve score: {average_mauve:.4f}\")\n",
    "print(f\"Average ROUGE-L score: {average_rough:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
