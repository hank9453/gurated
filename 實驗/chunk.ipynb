{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import fitz\n",
    "import os\n",
    "\n",
    "# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "embed_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "local_chunk_set = []\n",
    "\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    \"\"\"è®€å– PDF æª”æ¡ˆä¸¦ä¾æ“šæ–‡æœ¬é æ•¸è¿”å›å…¶å…§å®¹\"\"\"\n",
    "    page_content= []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        page_content.append({'page':page_num+1,'content':text})\n",
    "    doc.close()\n",
    "    return page_content\n",
    "\n",
    "\n",
    "def chunk_text(pages, chunk_size):\n",
    "    \"\"\"å°‡æ–‡æœ¬æ‹†åˆ†ç‚ºæŒ‡å®šå¤§å°çš„ chunksï¼Œå…è¨± chunk è·¨è¶Šé æ•¸ï¼Œä¸¦æ¨™è¨˜ chunk æ¶‰åŠçš„é æ•¸ç¯„åœ\"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_pages = set()\n",
    "    current_length = 0\n",
    "    max_token_length = chunk_size  \n",
    "\n",
    "    for page in pages:\n",
    "        text = page['content']\n",
    "        text_tokens = embed_model.tokenizer.tokenize(text)\n",
    "\n",
    "        while text_tokens:\n",
    "            space_left = max_token_length - current_length\n",
    "\n",
    "            # å¦‚æœç•¶å‰ chunk é‚„æœ‰ç©ºé–“\n",
    "            if space_left > 0:\n",
    "                tokens_to_add = text_tokens[:space_left]\n",
    "                text_tokens = text_tokens[space_left:]\n",
    "\n",
    "                current_chunk.extend(tokens_to_add)\n",
    "                current_pages.add(page['page'])\n",
    "                current_length += len(tokens_to_add)\n",
    "\n",
    "            # ç•¶ chunk æ»¿äº†ï¼Œå°±å­˜å…¥ chunksï¼Œä¸¦é‡ç½®è®Šæ•¸\n",
    "            if current_length >= max_token_length or (current_length > 0 and len(text_tokens) > 0):\n",
    "                chunks.append({\n",
    "                    'pages': sorted(current_pages),\n",
    "                    'content': embed_model.tokenizer.convert_tokens_to_string(current_chunk)\n",
    "                })\n",
    "                current_chunk = []\n",
    "                current_pages = set()\n",
    "                current_length = 0\n",
    "\n",
    "    # è™•ç†æœ€å¾Œä¸€å€‹æœªæ»¿çš„ chunk\n",
    "    if current_length > 0:\n",
    "        chunks.append({\n",
    "            'pages': sorted(current_pages),\n",
    "            'content': embed_model.tokenizer.convert_tokens_to_string(current_chunk)\n",
    "        })\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../pdf'\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŸå§‹çš„chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:371: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å„²å­˜ 119 ç­†è³‡æ–™åˆ° FAISS\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# å„²å­˜ embeddings é¿å…é‡è¤‡è¨ˆç®—\n",
    "embedding_cache = {}\n",
    "data = []\n",
    "\n",
    "# FAISS ç´¢å¼•åˆå§‹åŒ–ï¼ˆå‡è¨­ embedding ç¶­åº¦ç‚º 384ï¼‰\n",
    "EMBEDDING_DIM = 1024  # **è«‹æ ¹æ“šä½ çš„ embedding æ¨¡å‹æ”¹è®Šé€™å€‹æ•¸å€¼**\n",
    "index = faiss.IndexFlatL2(EMBEDDING_DIM)  # L2 è·é›¢ç´¢å¼•\n",
    "\n",
    "# å„²å­˜ FAISS ID å°æ‡‰çš„ metadata\n",
    "faiss_metadata = {}\n",
    "\n",
    "# è¨­å®šæ‰¹æ¬¡å¤§å°ï¼Œé¿å… OOM\n",
    "\n",
    "chunk_sizes = [128]  # æ¸¬è©¦ä¸åŒ chunk sizes\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    full_path = os.path.join(folder_path, pdf_file)\n",
    "    content = read_pdf(full_path)\n",
    "\n",
    "    for size in chunk_sizes:\n",
    "        chunks = chunk_text(content, size)\n",
    "\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            text = chunk[\"content\"]\n",
    "\n",
    "            # **ğŸš€ æª¢æŸ¥ embedding æ˜¯å¦å·²ç¶“è¨ˆç®—é**\n",
    "            if text in embedding_cache:\n",
    "                continue  # è·³éå·²è™•ç†éçš„æ–‡æœ¬\n",
    "            \n",
    "            # **ğŸš€ è¨ˆç®— embedding ä¸¦å­˜å…¥ cache**\n",
    "            embedding = embed_model.encode(text).astype(np.float32)\n",
    "            embedding_cache[text] = embedding\n",
    "\n",
    "            # **ğŸš€ å„²å­˜ metadata**\n",
    "            data.append({\n",
    "                \"File_Name\": pdf_file,\n",
    "                \"content\": text,\n",
    "                \"Page_Num\": ','.join(str(p) for p in chunk[\"pages\"])\n",
    "            })\n",
    "            index.add(np.array([embedding]))  # åŠ å…¥ FAISS\n",
    "            faiss_metadata[len(faiss_metadata)] = data[-1]  # FAISS ID å°æ‡‰ metadata\n",
    "\n",
    "# **ğŸš€ å„²å­˜ FAISS ç´¢å¼•**\n",
    "faiss.write_index(index, \"faiss_index.idx\")\n",
    "with open(\"faiss_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(faiss_metadata, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"å·²å„²å­˜ {len(faiss_metadata)} ç­†è³‡æ–™åˆ° FAISS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¨ˆç®— naive chunk çš„æº–ç¢ºåº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ç™»å…¥ã€Œæ•™è‚²æ©Ÿæ§‹è³‡å®‰é€šå ±å¹³å°ã€ã€‚\n",
      "2. é»é¸å·¦æ–¹åŠŸèƒ½åˆ—çš„ã€Œä¿®æ”¹è³‡å®‰é•·è³‡æ–™ã€ã€‚\n",
      "3. è¼¸å…¥å–®ä½çš„è³‡å®‰é•·ç›¸é—œé€£çµ¡è³‡è¨Š(åŒ…å«å§“åã€å…¬å‹™é›»è©±ã€å…¬å‹™é›»å­éƒµä»¶)ã€‚\n",
      "4. é»é¸ã€Œé€å‡ºã€ä»¥å„²å­˜è³‡æ–™ã€‚\n",
      "è³‡å®‰é•·çš„è¯çµ¡è³‡è¨Šéœ€è¦åŒ…å«è³‡å®‰é•·å§“åã€è³‡å®‰é•·å…¬å‹™é›»è©±åŠè³‡å®‰é•·å…¬å‹™é›»å­éƒµä»¶ã€‚\n",
      "åœ¨è³‡å®‰äº‹ä»¶ç™¼ç”Ÿæ™‚ï¼Œè‡ºç£å­¸è¡“ç¶²è·¯å„ç´šå­¸æ ¡æ‡‰å…ˆç¢ºèªäº‹ä»¶ï¼Œç¶“ç¢ºèªç‚ºè³‡å®‰äº‹ä»¶å¾Œï¼Œé ˆæ–¼1å°æ™‚å…§è‡³æ•™è‚²æ©Ÿæ§‹è³‡å®‰é€šå ±æ‡‰è®Šç¶²ç«™(https://info. cert.tanet.edu.tw)é€šå ±ç™»éŒ„è³‡å®‰äº‹ä»¶ï¼Œä¸¦éµå¾ªå„å–®ä½å…§éƒ¨å‚™ä»½ç®¡ç†è¾¦æ³•å•Ÿå‹•ç›¸é—œæ‡‰è®Šæªæ–½ã€‚\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 75\u001b[0m\n\u001b[0;32m     66\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mä½ æ˜¯ä¸€å€‹ AI åŠ©ç†ï¼Œå°ˆé–€æ ¹æ“šå®˜æ–¹æ–‡ä»¶å›ç­”è³‡å®‰ç›¸é—œå•é¡Œã€‚\u001b[39m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124mè«‹æ ¹æ“šæä¾›çš„æ–‡ä»¶å…§å®¹ä¾†ç”Ÿæˆç²¾ç¢ºä¸”ç°¡æ½”çš„ç­”æ¡ˆï¼Œä¸è¦åŠ å…¥é¡å¤–è³‡è¨Šã€‚\u001b[39m\n\u001b[0;32m     68\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124m**ç­”æ¡ˆï¼š**\u001b[39m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# ç”Ÿæˆå›ç­”\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m generated_answer \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_answer)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# è¨ˆç®— BLEU å’Œ ROUGE åˆ†æ•¸\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:387\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    384\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    385\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 387\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    388\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    389\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    390\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    393\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    394\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    395\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    396\u001b[0m         )\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    399\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:760\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    754\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    758\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    759\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:963\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    950\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    951\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     ]\n\u001b[1;32m--> 963\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    964\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    965\u001b[0m     )\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:784\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    776\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    781\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    783\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 784\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    785\u001b[0m                 prompts,\n\u001b[0;32m    786\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    787\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    788\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    789\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    790\u001b[0m             )\n\u001b[0;32m    791\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    792\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    793\u001b[0m         )\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\llms\\ollama.py:437\u001b[0m, in \u001b[0;36mOllama._generate\u001b[1;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    435\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 437\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    438\u001b[0m         prompt,\n\u001b[0;32m    439\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    440\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[0;32m    441\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    442\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    443\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    444\u001b[0m     )\n\u001b[0;32m    445\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\llms\\ollama.py:349\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    342\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    348\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    350\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    351\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[1;32mc:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\llms\\ollama.py:194\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[1;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    188\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    192\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    193\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[0;32m    195\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[0;32m    196\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    197\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    199\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:869\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[1;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \n\u001b[0;32m    864\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    867\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[0;32m    870\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode\n\u001b[0;32m    871\u001b[0m ):\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[1;32mc:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\utils.py:572\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[1;34m(iterator, r)\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    571\u001b[0m decoder \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetincrementaldecoder(r\u001b[38;5;241m.\u001b[39mencoding)(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 572\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m    573\u001b[0m     rv \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(chunk)\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rv:\n",
      "File \u001b[1;32mc:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:1206\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1203\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1208\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:1125\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1125\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from langchain.llms import Ollama\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer  # ç¢ºä¿å®‰è£ `rouge-score` å¥—ä»¶\n",
    "from sentence_transformers import SentenceTransformer  # FAISS éœ€è¦æ­¤æ¨¡å‹\n",
    "\n",
    "# åŠ è¼‰ FAISS Index å’Œ Metadata\n",
    "index = faiss.read_index(\"faiss_index.idx\")\n",
    "with open(\"faiss_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    faiss_metadata = json.load(f)\n",
    "\n",
    "embed_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "\n",
    "# è¨ˆç®— BLEU åˆ†æ•¸\n",
    "def calculate_blue_score(reference, candidate):\n",
    "    reference = [reference.split()]\n",
    "    candidate = candidate.split()\n",
    "    smoothing = SmoothingFunction().method1  # é¿å… 0 åˆ†æ•¸å•é¡Œ\n",
    "    score = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
    "    return score\n",
    "\n",
    "# è¨ˆç®— ROUGE åˆ†æ•¸\n",
    "def calculate_rough_score(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    return scores['rougeL'].fmeasure  # å– ROUGE-L åˆ†æ•¸\n",
    "\n",
    "# æœå°‹ FAISS Index\n",
    "def search_faiss(query_text, top_k=3):\n",
    "    \"\"\"æœå°‹æœ€ç›¸è¿‘çš„å‰ top_k ç­†è³‡æ–™\"\"\"\n",
    "    query_embedding = embed_model.encode(query_text).astype(np.float32)\n",
    "    query_embedding = np.expand_dims(query_embedding, axis=0)  # FAISS éœ€è¦ 2D é™£åˆ—\n",
    "\n",
    "    # **ğŸš€ æŸ¥è©¢ FAISS**\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        if str(idx) in faiss_metadata:\n",
    "            results.append(faiss_metadata[str(idx)])\n",
    "\n",
    "    return results\n",
    "\n",
    "# è®€å–æ¸¬è©¦å•é¡Œ\n",
    "with open('pdf_questions.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# åˆå§‹åŒ– Ollama å®¢æˆ¶ç«¯\n",
    "llm = Ollama(model=\"jcai/llama-3-taiwan-8b-instruct:q4_k_m\")\n",
    "\n",
    "win = 0\n",
    "loss = 0\n",
    "total_blue = 0\n",
    "total_rough = 0\n",
    "\n",
    "for each in data:\n",
    "    res = search_faiss(each['question'])\n",
    "    found = False\n",
    "\n",
    "    # æ•´ç† Context\n",
    "    context = \" \".join([metadata['content'] for metadata in res])\n",
    "\n",
    "    # LLM Prompt\n",
    "    prompt = f\"\"\"ä½ æ˜¯ä¸€å€‹ AI åŠ©ç†ï¼Œå°ˆé–€æ ¹æ“šå®˜æ–¹æ–‡ä»¶å›ç­”è³‡å®‰ç›¸é—œå•é¡Œã€‚\n",
    "    è«‹æ ¹æ“šæä¾›çš„æ–‡ä»¶å…§å®¹ä¾†ç”Ÿæˆç²¾ç¢ºä¸”ç°¡æ½”çš„ç­”æ¡ˆï¼Œä¸è¦åŠ å…¥é¡å¤–è³‡è¨Šã€‚\n",
    "\n",
    "    **å•é¡Œï¼š** {each['question']}\n",
    "    **åƒè€ƒå…§å®¹ï¼š** {context}\n",
    "    **ç­”æ¡ˆï¼š**\n",
    "    \"\"\"\n",
    "    \n",
    "    # ç”Ÿæˆå›ç­”\n",
    "    generated_answer = llm.invoke(prompt).strip()\n",
    "    print(generated_answer)\n",
    "    # è¨ˆç®— BLEU å’Œ ROUGE åˆ†æ•¸\n",
    "    blue_score = calculate_blue_score(each['answer'], generated_answer)\n",
    "    rough_score = calculate_rough_score(each['answer'], generated_answer)\n",
    "\n",
    "    total_blue += blue_score\n",
    "    total_rough += rough_score\n",
    "\n",
    "    # ç¢ºèªæ˜¯å¦åŒ¹é…æ–‡ä»¶åç¨±å’Œé ç¢¼\n",
    "    for metadata in res:\n",
    "        if metadata['File_Name'] == each['File_Name'] and str(each['Page_Num']) in map(str, metadata['Page_Num']):\n",
    "            win += 1\n",
    "            found = True\n",
    "            break  # æ‰¾åˆ°åŒ¹é…çš„å°±ä¸ç¹¼çºŒæª¢æŸ¥\n",
    "\n",
    "    if not found:\n",
    "        loss += 1\n",
    "\n",
    "# è¨ˆç®—æœ€çµ‚æŒ‡æ¨™\n",
    "accuracy = win / (win + loss) if (win + loss) > 0 else 0\n",
    "average_blue = total_blue / win if win > 0 else 0\n",
    "average_rough = total_rough / win if win > 0 else 0\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Average BLEU score: {average_blue:.4f}\")\n",
    "print(f\"Average ROUGE-L score: {average_rough:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¨ˆç®— element chunk çš„æº–ç¢ºåº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917910447761194\n"
     ]
    }
   ],
   "source": [
    "index = faiss.read_index(\"faiss_element_chunk_index.idx\")\n",
    "with open(\"faiss_element_chunk_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    faiss_metadata = json.load(f)\n",
    "def search_faiss(query_text, top_k=3):\n",
    "    \"\"\"æœå°‹æœ€ç›¸è¿‘çš„å‰ top_k ç­†è³‡æ–™\"\"\"\n",
    "    query_embedding = embed_model.encode(query_text).astype(np.float32)\n",
    "    query_embedding = np.expand_dims(query_embedding, axis=0)  # FAISS éœ€è¦ 2D é™£åˆ—\n",
    "\n",
    "    # **ğŸš€ æŸ¥è©¢ FAISS**\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        if str(idx) in faiss_metadata:\n",
    "            results.append(faiss_metadata[str(idx)])\n",
    "\n",
    "    return results\n",
    "\n",
    "with open('pdf_questions.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "win = 0\n",
    "loss = 0\n",
    "for each in data : \n",
    "\n",
    "\n",
    "    res = search_faiss(each['question'])\n",
    "    found = False\n",
    "    for metadata in res:\n",
    "        if metadata['File_Name'] == each['File_Name'] and str(each['Page_Num']) in  metadata['Page_Num']:\n",
    "            win += 1\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        loss += 1\n",
    "\n",
    "print(win/(win+loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
