{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0.dev20250303+cu128\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_list = [\n",
    "    \"BAAI/bge-reranker-v2-m3\",\n",
    "    \"BAAI/bge-reranker-large\",\n",
    "    \"BAAI/bge-reranker-base\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import fitz\n",
    "import numpy as np\n",
    "import os\n",
    "import faiss\n",
    "import json\n",
    "from FlagEmbedding import FlagReranker\n",
    "def read_pdf(pdf_path):\n",
    "    \"\"\"ËÆÄÂèñ PDF Ê™îÊ°à‰∏¶‰æùÊìöÊñáÊú¨È†ÅÊï∏ËøîÂõûÂÖ∂ÂÖßÂÆπ\"\"\"\n",
    "    page_content= []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        page_content.append({'page':page_num+1,'content':text})\n",
    "    doc.close()\n",
    "    return page_content\n",
    "\n",
    "\n",
    "def chunk_text(pages, chunk_size, embed_model):\n",
    "    \"\"\"Â∞áÊñáÊú¨ÊãÜÂàÜÁÇ∫ÊåáÂÆöÂ§ßÂ∞èÁöÑ chunksÔºåÂÖÅË®± chunk Ë∑®Ë∂äÈ†ÅÊï∏Ôºå‰∏¶Ê®ôË®ò chunk Ê∂âÂèäÁöÑÈ†ÅÊï∏ÁØÑÂúç\"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_pages = set()\n",
    "    current_length = 0\n",
    "    max_token_length = chunk_size # Á¢∫‰øù chunk_size ‰∏çË∂ÖÈÅé 512\n",
    "\n",
    "    for page in pages:\n",
    "        text = page['content']\n",
    "        text_tokens = embed_model.tokenizer.tokenize(text)\n",
    "\n",
    "        while text_tokens:\n",
    "            space_left = max_token_length - current_length\n",
    "\n",
    "            # Â¶ÇÊûúÁï∂Ââç chunk ÈÇÑÊúâÁ©∫Èñì\n",
    "            if space_left > 0:\n",
    "                tokens_to_add = text_tokens[:space_left]\n",
    "                text_tokens = text_tokens[space_left:]\n",
    "\n",
    "                current_chunk.extend(tokens_to_add)\n",
    "                current_pages.add(page['page'])\n",
    "                current_length += len(tokens_to_add)\n",
    "\n",
    "            # Áï∂ chunk Êªø‰∫ÜÔºåÂ∞±Â≠òÂÖ• chunksÔºå‰∏¶ÈáçÁΩÆËÆäÊï∏\n",
    "            if current_length >= max_token_length or (current_length > 0 and len(text_tokens) > 0):\n",
    "                chunks.append({\n",
    "                    'pages': sorted(current_pages),\n",
    "                    'content': embed_model.tokenizer.convert_tokens_to_string(current_chunk)\n",
    "                })\n",
    "                current_chunk = []\n",
    "                current_pages = set()\n",
    "                current_length = 0\n",
    "\n",
    "    # ËôïÁêÜÊúÄÂæå‰∏ÄÂÄãÊú™ÊªøÁöÑ chunk\n",
    "    if current_length > 0:\n",
    "        chunks.append({\n",
    "            'pages': sorted(current_pages),\n",
    "            'content': embed_model.tokenizer.convert_tokens_to_string(current_chunk)\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def search_faiss(query_text, embed_model,index,faiss_metadata,top_k=2):\n",
    "    \"\"\"ÊêúÂ∞ãÊúÄÁõ∏ËøëÁöÑÂâç top_k Á≠ÜË≥áÊñô\"\"\"\n",
    "    query_embedding = embed_model.encode(query_text).astype(np.float32)\n",
    "    query_embedding = np.expand_dims(query_embedding, axis=0)  # FAISS ÈúÄË¶Å 2D Èô£Âàó\n",
    "\n",
    "    # **üöÄ Êü•Ë©¢ FAISS**\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        if str(idx) in faiss_metadata:\n",
    "            results.append(faiss_metadata[str(idx)])\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â∑≤ÂÑ≤Â≠ò 4853 Á≠ÜË≥áÊñôÂà∞ FAISS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â∑≤ÂÑ≤Â≠ò 2461 Á≠ÜË≥áÊñôÂà∞ FAISS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â∑≤ÂÑ≤Â≠ò 1265 Á≠ÜË≥áÊñôÂà∞ FAISS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â∑≤ÂÑ≤Â≠ò 668 Á≠ÜË≥áÊñôÂà∞ FAISS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    chenk_szie  top_k                    model  win  loss       MRR  pre_time\n",
      "0           64   10.0  BAAI/bge-reranker-v2-m3  123    11  0.827114  0.102219\n",
      "1           64   10.0  BAAI/bge-reranker-large  124    10  0.802239  0.104230\n",
      "2           64   10.0   BAAI/bge-reranker-base  123    11  0.805970  0.076723\n",
      "3           64    NaN                No_Rerank  121    13  0.766169  0.028040\n",
      "4           64   20.0  BAAI/bge-reranker-v2-m3  123    11  0.827114  0.098115\n",
      "..         ...    ...                      ...  ...   ...       ...       ...\n",
      "59         512    NaN                No_Rerank  110    24  0.690299  0.026513\n",
      "60         512  100.0  BAAI/bge-reranker-v2-m3  125     9  0.875622  0.157992\n",
      "61         512  100.0  BAAI/bge-reranker-large  123    11  0.854478  0.161433\n",
      "62         512  100.0   BAAI/bge-reranker-base  125     9  0.858209  0.084926\n",
      "63         512    NaN                No_Rerank  110    24  0.690299  0.026586\n",
      "\n",
      "[64 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "model_performance = []\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "folder_path = '../pdf'\n",
    "\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]\n",
    "embed_model = SentenceTransformer(model_name ,trust_remote_code=True)\n",
    "EMBEDDING_DIM = embed_model.get_sentence_embedding_dimension()\n",
    "chunk_sizes = [64,128,256,512]  # Ê∏¨Ë©¶‰∏çÂêå chunk sizes\n",
    "top_kk = [10,20,50,100]\n",
    "\n",
    "for size in chunk_sizes:\n",
    "\n",
    "    faiss_metadata = {}\n",
    "    index = faiss.IndexFlatL2(EMBEDDING_DIM)  # L2 Ë∑ùÈõ¢Á¥¢Âºï\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        files_to_delete = ['faiss_metadata.json', 'faiss_index.idx']\n",
    "        for file in files_to_delete:\n",
    "            if os.path.exists(file):\n",
    "                os.remove(file)\n",
    "        full_path = os.path.join(folder_path, pdf_file)\n",
    "        content = read_pdf(full_path)\n",
    "        chunks = chunk_text(content, size,embed_model)\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            text = chunk[\"content\"]\n",
    "            embedding = embed_model.encode(text).astype(np.float32)\n",
    "            data = {\n",
    "                \"File_Name\": pdf_file,\n",
    "                \"content\": text,\n",
    "                \"Page_Num\": ','.join(str(p) for p in chunk[\"pages\"])\n",
    "            }\n",
    "            index.add(np.array([embedding]))  # Âä†ÂÖ• FAISS\n",
    "            faiss_metadata[len(faiss_metadata)] = data  # FAISS ID Â∞çÊáâ metadata\n",
    "\n",
    "    # **üöÄ ÂÑ≤Â≠ò FAISS Á¥¢Âºï**\n",
    "    faiss.write_index(index, \"faiss_index.idx\")\n",
    "    with open(\"faiss_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(faiss_metadata, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Â∑≤ÂÑ≤Â≠ò {len(faiss_metadata)} Á≠ÜË≥áÊñôÂà∞ FAISS\")\n",
    "    with open(\"faiss_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        faiss_metadata = json.load(f)\n",
    "    with open('pdf_questions.json', 'r', encoding='utf-8') as f:\n",
    "        pdf_questions = json.load(f)\n",
    "    for top_k in top_kk:\n",
    "        for reranker_name in reranker_list:\n",
    "            index = faiss.read_index(\"faiss_index.idx\")\n",
    "            start_time = time.time()\n",
    "            win = 0\n",
    "            loss = 0\n",
    "            MRR = 0\n",
    "            reranker_model = FlagReranker(reranker_name, use_fp16=True)\n",
    "            for query in pdf_questions:\n",
    "                results = search_faiss(query['question'], embed_model,index,faiss_metadata,top_k=10) \n",
    "                # Prepare pairs for reranking\n",
    "                pairs = []\n",
    "                for result in results:\n",
    "                    pairs.append([query['question'], result['content']])\n",
    "                \n",
    "                # Get reranking scores\n",
    "                if pairs:  # Only rerank if we have results\n",
    "                    scores = reranker_model.compute_score(pairs)\n",
    "                    for i, result in enumerate(results):\n",
    "                        result['score'] = scores[i].item() if hasattr(scores[i], \"item\") else scores[i]\n",
    "                    # Sort results by score in descending order\n",
    "                    results = sorted(results, key=lambda x: x['score'], reverse=True)\n",
    "                results = results[:3]\n",
    "                find = False\n",
    "                for idx, metadata in enumerate(results):\n",
    "                    if metadata['File_Name'] == query['File_Name'] and str(query['Page_Num']) in  metadata['Page_Num']:\n",
    "                        find = True\n",
    "                        win += 1\n",
    "                        MRR += 1/(idx+1)\n",
    "                        break\n",
    "                if not find:\n",
    "                    loss += 1\n",
    "            end_time = time.time() - start_time\n",
    "            model_performance.append({'chenk_szie':size,'top_k':top_k,'model':reranker_name,'win':win,'loss':loss,'MRR':MRR/len(pdf_questions) , 'pre_time':end_time/len(pdf_questions)})\n",
    "\n",
    "        \n",
    "        win = 0\n",
    "        loss = 0\n",
    "        MRR = 0\n",
    "        start_time = time.time()\n",
    "        for query in pdf_questions:\n",
    "            results = search_faiss(query['question'], embed_model,index,faiss_metadata,top_k=3)\n",
    "            find = False\n",
    "            for idx, metadata in enumerate(results):\n",
    "                if metadata['File_Name'] == query['File_Name'] and str(query['Page_Num']) in  metadata['Page_Num']:\n",
    "                    find = True\n",
    "                    win += 1\n",
    "                    MRR += 1/(idx+1)\n",
    "                    break\n",
    "            if not find:\n",
    "                loss += 1\n",
    "        end_time = time.time() - start_time\n",
    "        model_performance.append({'chenk_szie':size,'model':'No_Rerank','win':win,'loss':loss,'MRR':MRR/len(pdf_questions), 'pre_time':end_time/len(pdf_questions)})\n",
    "\n",
    "\n",
    "# Convert model_performance list to DataFrame for better visualization\n",
    "df_performance = pd.DataFrame(model_performance)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance: MRR vs Processing Time\n",
      "Model: BAAI/bge-reranker-v2-m3\n",
      "    pre_time       MRR  win_rate\n",
      "0   0.102219  0.827114  0.917910\n",
      "4   0.098115  0.827114  0.917910\n",
      "8   0.097561  0.827114  0.917910\n",
      "12  0.101139  0.827114  0.917910\n",
      "16  0.099310  0.863184  0.947761\n",
      "20  0.097893  0.863184  0.947761\n",
      "24  0.098960  0.863184  0.947761\n",
      "28  0.099929  0.863184  0.947761\n",
      "32  0.112484  0.864428  0.910448\n",
      "36  0.111528  0.864428  0.910448\n",
      "40  0.111581  0.864428  0.910448\n",
      "44  0.111754  0.864428  0.910448\n",
      "48  0.157325  0.875622  0.932836\n",
      "52  0.158638  0.875622  0.932836\n",
      "56  0.159356  0.875622  0.932836\n",
      "60  0.157992  0.875622  0.932836\n",
      "Model: BAAI/bge-reranker-large\n",
      "    pre_time       MRR  win_rate\n",
      "1   0.104230  0.802239  0.925373\n",
      "5   0.101680  0.802239  0.925373\n",
      "9   0.100950  0.802239  0.925373\n",
      "13  0.101010  0.802239  0.925373\n",
      "17  0.101822  0.851990  0.940299\n",
      "21  0.101033  0.851990  0.940299\n",
      "25  0.101453  0.851990  0.940299\n",
      "29  0.101848  0.851990  0.940299\n",
      "33  0.115263  0.822139  0.902985\n",
      "37  0.115916  0.822139  0.902985\n",
      "41  0.114805  0.822139  0.902985\n",
      "45  0.114969  0.822139  0.902985\n",
      "49  0.162192  0.854478  0.917910\n",
      "53  0.161375  0.854478  0.917910\n",
      "57  0.161038  0.854478  0.917910\n",
      "61  0.161433  0.854478  0.917910\n",
      "Model: BAAI/bge-reranker-base\n",
      "    pre_time       MRR  win_rate\n",
      "2   0.076723  0.805970  0.917910\n",
      "6   0.076026  0.805970  0.917910\n",
      "10  0.077388  0.805970  0.917910\n",
      "14  0.076202  0.805970  0.917910\n",
      "18  0.075735  0.853234  0.955224\n",
      "22  0.076214  0.853234  0.955224\n",
      "26  0.081054  0.853234  0.955224\n",
      "30  0.076675  0.853234  0.955224\n",
      "34  0.076371  0.844527  0.925373\n",
      "38  0.078617  0.844527  0.925373\n",
      "42  0.076582  0.844527  0.925373\n",
      "46  0.076257  0.844527  0.925373\n",
      "50  0.086003  0.858209  0.932836\n",
      "54  0.084614  0.858209  0.932836\n",
      "58  0.085098  0.858209  0.932836\n",
      "62  0.084926  0.858209  0.932836\n",
      "Model: No_Rerank\n",
      "    pre_time       MRR  win_rate\n",
      "3   0.028040  0.766169  0.902985\n",
      "7   0.027437  0.766169  0.902985\n",
      "11  0.027498  0.766169  0.902985\n",
      "15  0.027562  0.766169  0.902985\n",
      "19  0.027547  0.777363  0.895522\n",
      "23  0.027493  0.777363  0.895522\n",
      "27  0.027045  0.777363  0.895522\n",
      "31  0.027334  0.777363  0.895522\n",
      "35  0.026903  0.730100  0.858209\n",
      "39  0.026957  0.730100  0.858209\n",
      "43  0.026457  0.730100  0.858209\n",
      "47  0.027055  0.730100  0.858209\n",
      "51  0.026430  0.690299  0.820896\n",
      "55  0.026540  0.690299  0.820896\n",
      "59  0.026513  0.690299  0.820896\n",
      "63  0.026586  0.690299  0.820896\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Calculate win rate for each model (win / total)\n",
    "df_performance['win_rate'] = df_performance['win'] / (df_performance['win'] + df_performance['loss'])\n",
    "# Print performance visualizations data\n",
    "print(\"Model Performance: MRR vs Processing Time\")\n",
    "for model in df_performance['model'].unique():\n",
    "    mask = df_performance['model'] == model\n",
    "    print(f\"Model: {model}\")\n",
    "    print(df_performance[mask][['pre_time', 'MRR','win_rate','top_k','chenk_szie']])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
